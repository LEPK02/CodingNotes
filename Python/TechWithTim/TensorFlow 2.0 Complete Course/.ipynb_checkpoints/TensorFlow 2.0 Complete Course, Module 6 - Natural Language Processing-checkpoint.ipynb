{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd36f47",
   "metadata": {},
   "source": [
    "<p>Notes found <a href=\"https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true\">here</a></p>\n",
    "<p>Video found <a href=\"https://www.youtube.com/watch?v=tPYj3fFJGjk&list=WL&index=5&t=11941s&ab_channel=freeCodeCamp.org\">here</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d87589",
   "metadata": {},
   "source": [
    "<h1>Encoding Textual Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3c609",
   "metadata": {},
   "source": [
    "<p>Bag of Words</p>\n",
    "<p>Tracks frequency of words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1105d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}  # maps word to integer representing it\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "\n",
    "    words = text.lower().split(\" \")  # create a list of all of the words in the text; assume there is no grammar in our text for this example\n",
    "    bag = {}  # stores all of the encodings and their frequency\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding = vocab[word]  # get encoding from vocab\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding = word_encoding\n",
    "            word_encoding += 1\n",
    "    \n",
    "        if encoding in bag:\n",
    "            bag[encoding] += 1\n",
    "        else:\n",
    "            bag[encoding] = 1\n",
    "  \n",
    "    return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95f0ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
      "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
     ]
    }
   ],
   "source": [
    "# Limitation of bag of words: does not track order of words, and therefore the meaning of the sentencess\n",
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_bag = bag_of_words(positive_review)\n",
    "neg_bag = bag_of_words(negative_review)\n",
    "\n",
    "print(\"Positive:\", pos_bag)\n",
    "print(\"Negative:\", neg_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18f721",
   "metadata": {},
   "source": [
    "<p>Integer Encoding</p>\n",
    "<p>Tracks frequency and order of words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdc8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}  \n",
    "word_encoding = 1\n",
    "def one_hot_encoding(text):\n",
    "    global word_encoding\n",
    "\n",
    "    words = text.lower().split(\" \") \n",
    "    encoding = []  \n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            code = vocab[word]  \n",
    "            encoding.append(code) \n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding.append(word_encoding)\n",
    "            word_encoding += 1\n",
    "  \n",
    "    return encoding\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "encoding = one_hot_encoding(text)\n",
    "print(encoding)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18001dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
      "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
     ]
    }
   ],
   "source": [
    "# Tracks order but not meaning (e.g. synonyms/antonyms) of words\n",
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_encode = one_hot_encoding(positive_review)\n",
    "neg_encode = one_hot_encoding(negative_review)\n",
    "\n",
    "print(\"Positive:\", pos_encode)\n",
    "print(\"Negative:\", neg_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dca443",
   "metadata": {},
   "source": [
    "<p>Word Embedding</p>\n",
    "<p>Tracks frequency, order and meaning of words by encoding each word as a dense vector (see below)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c38ea",
   "metadata": {},
   "source": [
    "<h1>Recurrent Neural Networks (RNN)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea11082",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a27360",
   "metadata": {},
   "source": [
    "<p>Data Preprocessing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c798e169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "17473536/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d20b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)  # Trim reviews with more than 250 words\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)  # Pad reviews with less than 250 words with 0s (left-padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd61aa8",
   "metadata": {},
   "source": [
    "<p>Creating the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba13ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),  # Use word embedding\n",
    "    tf.keras.layers.LSTM(32),  # 32 is the number of dimensions the output has\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")  # Sigmoid restricts output to [0,1], where 0.5 is a neutral review, >0.5 is positive and <0.5 is a negative revivew\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3db9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          2834688   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb3882",
   "metadata": {},
   "source": [
    "<p>Training the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5aad0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 76s 119ms/step - loss: 0.4100 - acc: 0.8123 - val_loss: 0.3182 - val_acc: 0.8640\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 77s 123ms/step - loss: 0.2344 - acc: 0.9130 - val_loss: 0.3465 - val_acc: 0.8606\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 77s 124ms/step - loss: 0.1811 - acc: 0.9345 - val_loss: 0.2669 - val_acc: 0.8964\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=3, validation_split=0.2)  # Use 20% of the training data to evaluate and validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb960299",
   "metadata": {},
   "source": [
    "<p>Evaluating the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc7b9cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3032 - acc: 0.8753 0s - loss: 0.3037 - acc: \n",
      "[0.3032104969024658, 0.8753200173377991]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e2065",
   "metadata": {},
   "source": [
    "<p>Making Predictions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9bcd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 0us/step\n",
      "1654784/1641221 [==============================] - 1s 0us/step\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Preprocesses data to make sure word encoding is the same as the training dataset's\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)  # Split sentences into individual words (tokens)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]  # Use encoding (i.e. int to word) in training dataset\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]  # Returns movie review as list of words and pads it\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb99a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "# Decode function\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]\n",
    "  \n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca51dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82816947]\n",
      "[0.37610605]\n"
     ]
    }
   ],
   "source": [
    "# now time to make a prediction\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)  # Preprocesses data (integer encoding)\n",
    "    pred = np.zeros((1,250))  # Creates numpy array of 250 0s (fixed input length of 250 words)\n",
    "    pred[0] = encoded_text  # Insert text input into array\n",
    "    result = model.predict(pred)  # Make prediction\n",
    "    print(result[0])  # Print desired prediction\n",
    "\n",
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f35e92",
   "metadata": {},
   "source": [
    "<h2>RNN Play Generator</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62956719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b5706",
   "metadata": {},
   "source": [
    "<p>Load Dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b10348cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "573e0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9e531",
   "metadata": {},
   "source": [
    "<p>Data Preprocessing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1801df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9808a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "# Decode text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c4ae5",
   "metadata": {},
   "source": [
    "<p>Creating Training Examples</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26828578",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)  # Takes input of x chars and predicts 1 character\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9867914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching data into desired length\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e2ed0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each batch into input and outputs of length 100\n",
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80164720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1c70005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training batches (64 different sequences)\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a9bbd",
   "metadata": {},
   "source": [
    "<p>Building the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32dbf987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,  # Return intermediate stage at every step\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)  # Gives probability distribution of each unique character\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a33ba",
   "metadata": {},
   "source": [
    "<p>Creating a Loss Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26e66dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Shows output of the model\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6ecd340",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.82788807e-03  2.01575982e-04  5.03402250e-03 ... -9.42482322e-04\n",
      "    5.33416215e-03  4.83257184e-03]\n",
      "  [ 3.94581817e-03 -2.95584276e-03  4.01094649e-03 ... -7.62519136e-04\n",
      "    3.85253597e-03  2.01960467e-03]\n",
      "  [-1.24495942e-03 -4.33709333e-03  3.80271813e-03 ... -2.90203607e-03\n",
      "    1.31797837e-03  1.00400881e-04]\n",
      "  ...\n",
      "  [-2.90967338e-03 -1.66946258e-02 -1.74449303e-03 ...  3.98084382e-03\n",
      "   -3.47526977e-03 -7.74108805e-04]\n",
      "  [ 1.98594062e-04 -1.39400531e-02 -5.80338296e-03 ...  6.57944009e-03\n",
      "   -1.12151052e-03  1.05705438e-02]\n",
      "  [ 7.30427820e-03 -1.75384283e-02 -1.46656588e-03 ...  5.89273591e-03\n",
      "    3.52087012e-03  1.37759168e-02]]\n",
      "\n",
      " [[-9.73303220e-04 -3.31859570e-04  8.12452810e-04 ...  4.92147077e-03\n",
      "    2.87920237e-03  3.16991704e-03]\n",
      "  [ 3.32242460e-04 -7.78404158e-03  4.82773583e-04 ...  5.18790819e-03\n",
      "   -6.63016271e-03  9.11252014e-03]\n",
      "  [-2.69867014e-03 -7.61294039e-03  2.14320002e-03 ...  1.60300476e-03\n",
      "   -6.69447333e-03  4.19102330e-03]\n",
      "  ...\n",
      "  [-1.00484751e-02 -1.95039026e-02  5.85971400e-04 ... -4.64540999e-03\n",
      "    1.34974131e-02 -9.71234217e-03]\n",
      "  [-1.14780534e-02 -2.12131571e-02  1.58668880e-03 ... -5.81314322e-03\n",
      "    6.53952779e-03 -4.36437875e-03]\n",
      "  [-7.71360565e-03 -1.88130103e-02  5.07693226e-03 ... -4.24860045e-03\n",
      "    1.22575723e-02  2.04748753e-03]]\n",
      "\n",
      " [[ 2.63279490e-03  2.34646979e-03  4.48522111e-03 ... -1.12000608e-03\n",
      "    7.90808722e-03  1.81993819e-04]\n",
      "  [-2.03046366e-03 -1.01063412e-03  4.79460089e-03 ... -3.58105032e-03\n",
      "    5.22354525e-03 -1.69130205e-03]\n",
      "  [-4.45166277e-03 -4.83036879e-03 -8.13309161e-04 ... -1.89176947e-03\n",
      "    4.27178945e-03 -4.63879493e-04]\n",
      "  ...\n",
      "  [-8.57047923e-03 -5.87999541e-03 -7.75427092e-03 ...  4.44239378e-03\n",
      "   -3.02806217e-03  3.57190683e-03]\n",
      "  [-5.51596191e-03 -2.64039449e-03 -1.77822681e-03 ...  3.51036969e-03\n",
      "    2.34523020e-03  7.55584659e-03]\n",
      "  [-3.89469834e-03 -7.59382593e-03 -1.06636323e-02 ...  5.02582313e-03\n",
      "    7.34400877e-04  1.02734854e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.69745740e-03 -4.26713470e-03  2.66750576e-03 ... -3.38137196e-03\n",
      "   -6.55361358e-03  1.46210147e-03]\n",
      "  [ 2.81747058e-03 -7.86444917e-03  4.84282104e-03 ... -4.00550431e-03\n",
      "   -8.05947371e-03  3.09733232e-03]\n",
      "  [-6.80473517e-04 -8.58087186e-03  5.82069950e-03 ... -5.26512135e-03\n",
      "   -7.08336476e-03  4.77062305e-04]\n",
      "  ...\n",
      "  [-1.08360462e-02 -1.49149895e-02 -2.88301427e-03 ...  1.06451847e-03\n",
      "    1.01642217e-02  4.26016143e-03]\n",
      "  [-4.84653655e-03 -1.87742021e-02 -4.17922065e-03 ...  1.65907899e-04\n",
      "    9.46798269e-03  1.39125902e-03]\n",
      "  [-6.70263544e-03 -1.91964936e-02 -8.63216259e-03 ...  7.50841806e-04\n",
      "    5.83894784e-03  3.35531053e-03]]\n",
      "\n",
      " [[ 1.00100040e-03 -5.42243593e-04 -5.70540037e-03 ...  3.29535268e-03\n",
      "    2.76022055e-03  4.13812324e-03]\n",
      "  [ 9.31680435e-04 -1.97379268e-03 -2.10693339e-03 ...  4.56649438e-03\n",
      "    3.33213573e-03  3.46875284e-03]\n",
      "  [-2.73628905e-03 -3.45842540e-03 -1.30281341e-03 ...  7.04755634e-03\n",
      "    5.72615955e-03  8.42675287e-03]\n",
      "  ...\n",
      "  [ 1.37752655e-03 -6.13380282e-04  3.13901296e-03 ...  1.14423092e-02\n",
      "    1.07077742e-02  6.26165373e-03]\n",
      "  [ 8.50065704e-03 -7.35634379e-03  6.21600635e-03 ...  9.44612082e-03\n",
      "    1.10256048e-02  8.70784931e-03]\n",
      "  [ 7.38546997e-03 -7.48845283e-03 -1.56244729e-03 ...  1.15024801e-02\n",
      "    1.13826254e-02  9.05558653e-03]]\n",
      "\n",
      " [[ 3.25803040e-03 -3.68985347e-03  2.71209632e-03 ... -1.48843334e-03\n",
      "   -3.11206840e-03  2.74348259e-03]\n",
      "  [ 7.97700137e-04 -6.23422815e-03  4.84592933e-03 ... -4.02069744e-03\n",
      "   -8.32867064e-03  4.09442931e-03]\n",
      "  [-5.18386532e-03 -8.42079613e-03  6.14849757e-03 ... -5.29925991e-03\n",
      "   -7.30881840e-03 -5.41936141e-04]\n",
      "  ...\n",
      "  [-1.74103603e-02 -1.00804889e-03 -1.49948895e-03 ... -2.40644044e-03\n",
      "    7.08586071e-03  3.28267924e-06]\n",
      "  [-1.62903350e-02 -4.24919557e-03 -3.11874319e-04 ... -4.64554876e-03\n",
      "    4.50769579e-03 -1.84467132e-03]\n",
      "  [-1.23468135e-02 -4.09997068e-03  3.07715521e-03 ... -6.98064733e-03\n",
      "    9.99391265e-03 -5.14291134e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5021b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 0.00182789  0.00020158  0.00503402 ... -0.00094248  0.00533416\n",
      "   0.00483257]\n",
      " [ 0.00394582 -0.00295584  0.00401095 ... -0.00076252  0.00385254\n",
      "   0.0020196 ]\n",
      " [-0.00124496 -0.00433709  0.00380272 ... -0.00290204  0.00131798\n",
      "   0.0001004 ]\n",
      " ...\n",
      " [-0.00290967 -0.01669463 -0.00174449 ...  0.00398084 -0.00347527\n",
      "  -0.00077411]\n",
      " [ 0.00019859 -0.01394005 -0.00580338 ...  0.00657944 -0.00112151\n",
      "   0.01057054]\n",
      " [ 0.00730428 -0.01753843 -0.00146657 ...  0.00589274  0.00352087\n",
      "   0.01377592]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d08ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 1.8278881e-03  2.0157598e-04  5.0340225e-03 -1.1198933e-04\n",
      " -2.5543952e-03  1.8606276e-03 -1.2137231e-03 -3.5601272e-04\n",
      "  4.1470136e-03  1.2043944e-03 -4.0052151e-03  1.8165645e-03\n",
      " -2.4454659e-03 -1.7880998e-04  2.2015984e-03  1.7457247e-03\n",
      "  3.6166115e-03  5.1432168e-03 -1.3972995e-03 -3.1049466e-03\n",
      " -2.4225553e-03  1.7501978e-03 -7.7822793e-04 -1.5873162e-03\n",
      " -3.3104816e-04  5.1440187e-03  5.9586451e-03  5.2843406e-03\n",
      "  8.0963841e-04  2.2171917e-03  1.9714087e-03  6.9209440e-03\n",
      "  3.9468948e-03  1.3430731e-03  6.9568260e-04 -1.5013647e-03\n",
      "  3.7770809e-03  4.8422916e-03  2.1085446e-04  8.5316377e-04\n",
      " -2.4801707e-03 -1.6185853e-03 -1.1221125e-03  4.1194800e-03\n",
      "  3.6000230e-04 -5.4782983e-03 -1.1672035e-03 -3.8305970e-03\n",
      "  9.1479160e-07  1.9324326e-04 -1.7411442e-04 -8.5141824e-04\n",
      " -1.3901860e-03 -3.3946878e-03 -1.7564500e-03 -1.9955414e-03\n",
      "  5.6158807e-03 -4.1949796e-05 -3.1693918e-03  2.6363947e-03\n",
      " -8.0230529e-04 -5.2355505e-03 -9.4248232e-04  5.3341622e-03\n",
      "  4.8325718e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a3ff6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zycm?HXobuLxv\\n XI!rsbB!EXNtSzmPrOrzzqncp!aa,;kVM-Z.GNgXyehpx,BvUxy!SC PfBdWfEUk;zgMlwMWcEzLuTmIaP$&s'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "260d7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4edd5",
   "metadata": {},
   "source": [
    "<p>Compiling the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "936147da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8f69c",
   "metadata": {},
   "source": [
    "<p>Creating Checkpoints</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dda9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALlows model to create and load checkpoints while it trains\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28527a8",
   "metadata": {},
   "source": [
    "<p>Training the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79c3b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1298s 8s/step - loss: 2.5432\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ce323",
   "metadata": {},
   "source": [
    "<p>Loading the Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e64d305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bd5a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint that stores the models weights\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e7a27d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-285931e0b97f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load specified checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2327\u001b[0m           'True when by_name is True.')\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m     \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m   3006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3007\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3008\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3009\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[0;32m    321\u001b[0m           filepath.endswith('.hdf5'))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "# Load specified checkpoint\n",
    "checkpoint_num = 1\n",
    "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22e004",
   "metadata": {},
   "source": [
    "<p>Generating Text</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1e1a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "          # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "529005b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken,\n",
      "To il, but that poobtur: not and peat nith oncy\n",
      "You rowit ins tol prouvee thes and\n",
      "thal't ve kins us the diest ofe, hes prowand and lent:\n",
      "Thic Caylf usfouk saun is I wond\n",
      "n'tongut on ssards ke burlins\n",
      "As naveer of is davp ce itein thee of thinkn: er.\n",
      "\n",
      "MANIM:\n",
      "\n",
      "ATI henrent wires tot with and that end shared fpracioldod'suld a pord ont,\n",
      "Whacids shave.\n",
      "\n",
      "GOO:\n",
      "Neners, baid and blongilk so\n",
      "The hathork live trotes of old Wiin,\n",
      "Then' Is plarders oro bad\n",
      "ISAd ant amant ream thiu.\n",
      "\n",
      "Firs UCILEN:\n",
      "Aswer, Lepa: on- tere and nod the farld, theur be of an memer will's\n",
      "Dither but atow tand and porolle ond now our our\n",
      "The cor of frated and ty lavers thes, Oull to anto, and\n",
      "Thet'st lefpire be corg thale red is siris not hiple in.\n",
      "\n",
      "CDUCINI:\n",
      "I play, wnit orie reer the so! prricheransing thit.\n",
      "\n",
      "HANBES:\n",
      "Saving \n"
     ]
    }
   ],
   "source": [
    "# inp = input(\"Type a starting string: \")\n",
    "inp = \"chicken\"\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
